# Comparing `tmp/simsi_transfer-0.6.0-py3-none-any.whl.zip` & `tmp/simsi_transfer-0.6.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 7752666 bytes, number of entries: 65
+Zip file size: 7752845 bytes, number of entries: 65
 -rw-r--r--  2.0 unx    11355 b- defN 80-Jan-01 00:00 LICENSE
 -rw-r--r--  2.0 unx     6510 b- defN 80-Jan-01 00:00 THERMO_LICENSE
 -rw-r--r--  2.0 unx     1576 b- defN 80-Jan-01 00:00 simsi_transfer/__init__.py
 -rw-r--r--  2.0 unx      166 b- defN 80-Jan-01 00:00 simsi_transfer/__main__.py
--rw-r--r--  2.0 unx     7303 b- defN 80-Jan-01 00:00 simsi_transfer/command_line_interface.py
+-rw-r--r--  2.0 unx     7554 b- defN 80-Jan-01 00:00 simsi_transfer/command_line_interface.py
 -rw-r--r--  2.0 unx    15113 b- defN 80-Jan-01 00:00 simsi_transfer/evidence.py
--rw-r--r--  2.0 unx     7647 b- defN 80-Jan-01 00:00 simsi_transfer/main.py
+-rw-r--r--  2.0 unx     7734 b- defN 80-Jan-01 00:00 simsi_transfer/main.py
 -rw-r--r--  2.0 unx     3101 b- defN 80-Jan-01 00:00 simsi_transfer/maracluster.py
 -rw-r--r--  2.0 unx     5847 b- defN 80-Jan-01 00:00 simsi_transfer/maxquant.py
 -rw-r--r--  2.0 unx     2090 b- defN 80-Jan-01 00:00 simsi_transfer/merging_functions.py
 -rw-r--r--  2.0 unx     4374 b- defN 80-Jan-01 00:00 simsi_transfer/simsi_output.py
 -rw-r--r--  2.0 unx     5412 b- defN 80-Jan-01 00:00 simsi_transfer/thermo_raw.py
--rw-r--r--  2.0 unx     9681 b- defN 80-Jan-01 00:00 simsi_transfer/tmt_processing.py
+-rw-r--r--  2.0 unx    10424 b- defN 80-Jan-01 00:00 simsi_transfer/tmt_processing.py
 -rw-r--r--  2.0 unx    11097 b- defN 80-Jan-01 00:00 simsi_transfer/transfer.py
 -rw-r--r--  2.0 unx    33200 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/AWS.Logger.Core.dll
 -rw-r--r--  2.0 unx     5384 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/AWS.Logger.Core.pdb
 -rw-r--r--  2.0 unx   136192 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.CloudWatchLogs.dll
 -rw-r--r--  2.0 unx    73260 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.CloudWatchLogs.pdb
 -rw-r--r--  2.0 unx   904704 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.Core.dll
 -rw-r--r--  2.0 unx   245016 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.Core.pdb
@@ -56,12 +56,12 @@
 -rw-r--r--  2.0 unx   480768 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/nunit.framework.dll
 -rw-r--r--  2.0 unx   177152 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/testcentric.engine.metadata.dll
 -rw-r--r--  2.0 unx    69632 b- defN 80-Jan-01 00:00 simsi_transfer/utils/ThermoRawFileParser/zlib.net.dll
 -rwxr-xr-x  2.0 unx  7096664 b- defN 80-Jan-01 00:00 simsi_transfer/utils/maracluster/linux64/maracluster
 -rw-r--r--  2.0 unx  5667840 b- defN 80-Jan-01 00:00 simsi_transfer/utils/maracluster/win64/maracluster.exe
 -rw-r--r--  2.0 unx      840 b- defN 80-Jan-01 00:00 simsi_transfer/utils/subprocess_with_logger.py
 -rw-r--r--  2.0 unx     3366 b- defN 80-Jan-01 00:00 simsi_transfer/utils/utils.py
--rw-r--r--  2.0 unx    11355 b- defN 80-Jan-01 00:00 simsi_transfer-0.6.0.dist-info/LICENSE
-?rw-r--r--  2.0 unx       83 b- defN 16-Jan-01 00:00 simsi_transfer-0.6.0.dist-info/WHEEL
-?rw-r--r--  2.0 unx     4731 b- defN 16-Jan-01 00:00 simsi_transfer-0.6.0.dist-info/METADATA
-?rw-r--r--  2.0 unx     7099 b- defN 16-Jan-01 00:00 simsi_transfer-0.6.0.dist-info/RECORD
-65 files, 22238029 bytes uncompressed, 7740938 bytes compressed:  65.2%
+-rw-r--r--  2.0 unx    11355 b- defN 80-Jan-01 00:00 simsi_transfer-0.6.1.dist-info/LICENSE
+?rw-r--r--  2.0 unx       83 b- defN 16-Jan-01 00:00 simsi_transfer-0.6.1.dist-info/WHEEL
+?rw-r--r--  2.0 unx     4731 b- defN 16-Jan-01 00:00 simsi_transfer-0.6.1.dist-info/METADATA
+?rw-r--r--  2.0 unx     7100 b- defN 16-Jan-01 00:00 simsi_transfer-0.6.1.dist-info/RECORD
+65 files, 22239111 bytes uncompressed, 7741117 bytes compressed:  65.2%
```

## zipnote {}

```diff
@@ -177,20 +177,20 @@
 
 Filename: simsi_transfer/utils/subprocess_with_logger.py
 Comment: 
 
 Filename: simsi_transfer/utils/utils.py
 Comment: 
 
-Filename: simsi_transfer-0.6.0.dist-info/LICENSE
+Filename: simsi_transfer-0.6.1.dist-info/LICENSE
 Comment: 
 
-Filename: simsi_transfer-0.6.0.dist-info/WHEEL
+Filename: simsi_transfer-0.6.1.dist-info/WHEEL
 Comment: 
 
-Filename: simsi_transfer-0.6.0.dist-info/METADATA
+Filename: simsi_transfer-0.6.1.dist-info/METADATA
 Comment: 
 
-Filename: simsi_transfer-0.6.0.dist-info/RECORD
+Filename: simsi_transfer-0.6.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## simsi_transfer/command_line_interface.py

```diff
@@ -125,27 +125,27 @@
 
     return args
 
 
 def get_input_folders(args):
     if args.meta_input_file:
         if args.raw_folder:
-            logging.error("Cannot use the --raw_folder and --meta_input_file parameters at the same time.")
+            raise ValueError("Cannot use the --raw_folder and --meta_input_file parameters at the same time.")
         if args.mq_txt_folder:
-            logging.error("Cannot use the --mq_txt_folder and --meta_input_file parameters at the same time.")
+            raise ValueError("Cannot use the --mq_txt_folder and --meta_input_file parameters at the same time.")
 
         meta_input_df = pd.read_csv(args.meta_input_file, sep='\t')
         meta_input_df.columns = ['mq_txt_folder', 'raw_folder', 'tmt_correction_file'][:len(meta_input_df.columns)]
         if 'tmt_correction_file' not in meta_input_df.columns:
             meta_input_df['tmt_correction_file'] = args.tmt_reporter_correction_file
     else:
         if not args.raw_folder:
-            logging.error("Missing --raw_folder argument")
+            raise ValueError("Missing --raw_folder argument")
         if not args.mq_txt_folder:
-            logging.error("Missing --mq_txt_folder argument")
+            raise ValueError("Missing --mq_txt_folder argument")
 
         meta_input_df = pd.DataFrame(
             list(zip([args.mq_txt_folder], [args.raw_folder], [args.tmt_reporter_correction_file])),
             columns=['mq_txt_folder', 'raw_folder', 'tmt_correction_file'])
 
     return meta_input_df
 
@@ -153,15 +153,24 @@
 def parse_stringencies(stringencies):
     if stringencies == '':
         stringencies = [20, 15, 10]
     else:
         try:
             stringencies = [int(i) for i in stringencies.split(',')]
         except ValueError:
-            logger.error(
+            raise ValueError(
                 'This is not a stringency list. Please input the list in the following format: 30,25,20,15,10,5')
 
     return stringencies
 
 
+def parse_tmt_ms_level(extraction_level: str):
+    if extraction_level == "ms2":
+        return 2
+    elif extraction_level == "ms3":
+        return 3
+    else:
+        raise ValueError("--tmt_ms_level should be either ms2 or ms3.")
+
+
 if __name__ == '__main__':
     raise NotImplementedError('Do not run this script.')
```

## simsi_transfer/main.py

```diff
@@ -20,14 +20,15 @@
 
 
 def main(argv):
     args = cli.parse_args(argv)
 
     pvals = cli.parse_stringencies(args.stringencies)
     meta_input_df = cli.get_input_folders(args)
+    tmt_ms_level = cli.parse_tmt_ms_level(args.tmt_ms_level)
 
     raw_folders = utils.convert_to_path_list(meta_input_df['raw_folder'])
     mq_txt_folders = utils.convert_to_path_list(meta_input_df['mq_txt_folder'])
     tmt_correction_files = utils.convert_to_path_list(meta_input_df['tmt_correction_file'])
 
     args.output_folder.mkdir(parents=True, exist_ok=True)
     args.cache_folder.mkdir(parents=True, exist_ok=True)
@@ -49,15 +50,15 @@
     logger.info(f"MaxQuant txt folder = {mq_txt_folders}")
     logger.info(f"Raw file folder = {raw_folders}")
     logger.info(f"Stringencies = {','.join(map(str, pvals))}")
     logger.info(f"Output folder = {args.output_folder}")
     logger.info(f"Cache folder = {args.cache_folder}")
     logger.info(f"Number of threads = {args.num_threads}")
     logger.info(f"TMT correction file = {tmt_correction_files}")
-    logger.info(f"TMT MS level = {args.tmt_ms_level}")
+    logger.info(f"TMT MS level = {tmt_ms_level}")
     logger.info('')
 
     logger.info(f'Starting SIMSI-Transfer')
     logger.info('')
 
     logger.info(f'Retrieving .raw files')
     meta_input_df['raw_files'] = meta_input_df['raw_folder'].apply(raw.get_raw_files)
@@ -89,15 +90,15 @@
             f'The raw files listed as input and the raw files in the MaxQuant search results are not the same!')
 
     if args.tmt_requantify:
         logger.info(f'Extracting correct reporter ion intensities from .mzML files')
         extracted_folder = args.cache_folder / Path('extracted')
         tmt_processing.extract_tmt_reporters(mzml_files=mzml_files, output_path=extracted_folder,
                                              correction_factor_paths=correction_factor_paths, plex=plex,
-                                             num_threads=args.num_threads)
+                                             extraction_level=tmt_ms_level, num_threads=args.num_threads)
 
         corrected_tmt = tmt_processing.assemble_corrected_tmt_table(mzml_files, extracted_folder, plex)
         msmsscans_mq = tmt_processing.merge_with_corrected_tmt(msmsscans_mq, corrected_tmt)
 
     logger.info(f'Reading in MaxQuant msms.txt file')
     msms_mq = utils.process_and_concat(mq_txt_folders, mq.read_msms_txt)
     if args.filter_decoys:
```

## simsi_transfer/tmt_processing.py

```diff
@@ -10,18 +10,20 @@
 
 from .utils import utils
 
 logger = logging.getLogger(__name__)
 
 
 def get_tmt_columns(plex):
-    return [f'raw_TMT{i}' for i in range(1, plex + 3)], [f'corr_TMT{i}' for i in range(1, plex + 1)]
+    return [f"raw_TMT{i}" for i in range(1, plex + 3)], [
+        f"corr_TMT{i}" for i in range(1, plex + 1)
+    ]
 
 
-def get_correction_factors(correction_factor_path: Path, plex_size):
+def get_correction_factors(correction_factor_path: Path, plex_size: int):
     # correction = np.array([[100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # 126 C Tag
     #                        [0.0, 100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # 127 N Tag
     #                        [0.0, 0.0, 100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # 127 C Tag
     #                        [0.0, 0.0, 0.0, 100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # 128 N Tag
     #                        [0.0, 0.0, 0.0, 0.0, 100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # 128 C Tag
     #                        [0.0, 0.0, 0.0, 0.0, 0.0, 100, 0.0, 0.0, 0.0, 0.0, 0.0],  # 129 N Tag
     #                        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100, 0.0, 0.0, 0.0, 0.0],  # 129 C Tag
@@ -33,131 +35,193 @@
     #                        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # 132 C Overflow
     #                        ])
     correction = np.zeros(shape=(plex_size + 2, plex_size))
     for i in range(correction.shape[1]):
         correction[i, i] = 100
 
     # Theoretical TMT Masses in m/z; same for standard TMT and TMTpro, different for sixplex though!
-    all_tmt_masses = np.array([126.127726, 127.124761, 127.131081, 128.128116, 128.134436, 129.131471,
-                               129.137790, 130.134825, 130.141145, 131.138180, 131.144499, 132.141535,
-                               132.147855, 133.144890, 133.151210, 134.148245, 134.154565, 135.151600])
+    all_tmt_masses = np.array(
+        [
+            126.127726,
+            127.124761,
+            127.131081,
+            128.128116,
+            128.134436,
+            129.131471,
+            129.137790,
+            130.134825,
+            130.141145,
+            131.138180,
+            131.144499,
+            132.141535,
+            132.147855,
+            133.144890,
+            133.151210,
+            134.148245,
+            134.154565,
+            135.151600,
+        ]
+    )
 
-    tmt_masses = all_tmt_masses[:plex_size + 2]
+    tmt_masses = all_tmt_masses[: plex_size + 2]
     if plex_size == 6:
         tmt_masses = np.array([all_tmt_masses[i] for i in [0, 1, 4, 5, 8, 9, 10, 11]])
 
     np.set_printoptions(linewidth=200)
     if correction_factor_path.is_file():
-        correction_dataframe = pd.read_csv(correction_factor_path, sep='\t')
+        correction_dataframe = pd.read_csv(correction_factor_path, sep="\t")
 
         for i in range(plex_size):
             if i not in [0, 1, 2, 3]:
-                correction[i - 4, i] = correction_dataframe.iloc[i]['Correction factor -2 [%]']
+                correction[i - 4, i] = correction_dataframe.iloc[i][
+                    "Correction factor -2 [%]"
+                ]
             if i not in [0, 1]:
-                correction[i - 2, i] = correction_dataframe.iloc[i]['Correction factor -1 [%]']
-            correction[i + 2, i] = correction_dataframe.iloc[i]['Correction factor +1 [%]']
+                correction[i - 2, i] = correction_dataframe.iloc[i][
+                    "Correction factor -1 [%]"
+                ]
+            correction[i + 2, i] = correction_dataframe.iloc[i][
+                "Correction factor +1 [%]"
+            ]
             if i not in [plex_size - 1, plex_size - 2, plex_size - 3]:
-                correction[i + 4, i] = correction_dataframe.iloc[i]['Correction factor +2 [%]']
+                correction[i + 4, i] = correction_dataframe.iloc[i][
+                    "Correction factor +2 [%]"
+                ]
     elif str(correction_factor_path) != ".":
-        logger.warning(f"Could not find reporter ion correction file at {str(correction_factor_path)}, no correction factors will be applied")
+        logger.warning(
+            f"Could not find reporter ion correction file at {str(correction_factor_path)}, no correction factors will be applied"
+        )
 
     # Normalize correction factors
-    correction_normalized = (correction / correction.sum(axis=0))
+    correction_normalized = correction / correction.sum(axis=0)
     return tmt_masses, correction_normalized
 
 
-def extract_tmt_reporters(mzml_files: List[Path], output_path: Path, correction_factor_paths: List[Path], plex: int,
-                          num_threads: int = 1, extraction_level: int = 3):
+def extract_tmt_reporters(
+    mzml_files: List[Path],
+    output_path: Path,
+    correction_factor_paths: List[Path],
+    plex: int,
+    extraction_level: int,
+    num_threads: int = 1,
+):
     """
     Takes about 1.5 minute for a 700MB file with 40k MS2 scans
     """
     if not output_path.is_dir():
         output_path.mkdir(parents=True)
 
     if num_threads > 1:
         from job_pool import JobPool
+
         processing_pool = JobPool(processes=num_threads, write_progress_to_logger=True)
+
     for mzml_file, correction_factor_path in zip(mzml_files, correction_factor_paths):
         args = (mzml_file, output_path, correction_factor_path, extraction_level, plex)
         if num_threads > 1:
             processing_pool.applyAsync(extract_and_correct_reporters, args)
         else:
             extract_and_correct_reporters(*args)
 
     if num_threads > 1:
         processing_pool.checkPool()
 
 
-def extract_and_correct_reporters(mzml_file, output_path, correction_factor_path, extraction_level, plex):
-    tmt_masses, correction_normalized = get_correction_factors(correction_factor_path, plex_size=plex)
+def extract_and_correct_reporters(
+    mzml_file: Path,
+    output_path: str,
+    correction_factor_path: Path,
+    extraction_level: int,
+    plex: int,
+):
+    tmt_masses, correction_normalized = get_correction_factors(
+        correction_factor_path, plex_size=plex
+    )
 
     tolerance = 6 * 1e-3 / 2
     tmt_upper = tmt_masses + tolerance
     tmt_lower = tmt_masses - tolerance
 
     tmt_raw_columns, tmt_corrected_columns = get_tmt_columns(plex)
 
-    convert_dict_raw = {k: 'float32' for k in tmt_raw_columns}
-    convert_dict_corr = {k: 'float32' for k in tmt_corrected_columns}
-    convert_dict_other = {'raw_file': 'str', 'scanID': 'int32'}
+    convert_dict_raw = {k: "float32" for k in tmt_raw_columns}
+    convert_dict_corr = {k: "float32" for k in tmt_corrected_columns}
+    convert_dict_other = {"raw_file": "str", "scanID": "int32"}
     convert_dict = {**convert_dict_raw, **convert_dict_corr, **convert_dict_other}
     dfcol = convert_dict.keys()
 
     output_file = get_extracted_tmt_file_name(output_path, mzml_file)
     if Path(output_file).is_file():
-        logger.debug(f"Found extracted reporter ions at {output_file}, skipping extraction")
+        logger.debug(
+            f"Found extracted reporter ions at {output_file}, skipping extraction"
+        )
         return
 
-    logger.info('Performing extraction for ' + mzml_file.name)
+    logger.info("Performing extraction for " + mzml_file.name)
     fileframe = pd.DataFrame(columns=dfcol)
 
     seen_scan_ids = set()
     with mzml.read(str(mzml_file)) as reader:
         for i, item in enumerate(reader):
-            if item['ms level'] != extraction_level:
+            if item["ms level"] != extraction_level:
                 continue
 
-            scanseries = pd.Series(index=dfcol, dtype='float32')
+            scanseries = pd.Series(index=dfcol, dtype="float32")
 
             if extraction_level == 2:
-                scanseries['scanID'] = re.search(r'scan=(\d+)', item['id'])[1]
+                scanseries["scanID"] = re.search(r"scan=(\d+)", item["id"])[1]
             else:
                 # supposed to find parent MS2 spectrum for MS3 by looking into precursorList/precursor/spectrumRef
-                scanseries['scanID'] = re.search(r'scan=(\d+)', item['precursorList']['precursor'][0]['spectrumRef'])[1]
+                scanseries["scanID"] = re.search(
+                    r"scan=(\d+)", item["precursorList"]["precursor"][0]["spectrumRef"]
+                )[1]
 
-            if scanseries['scanID'] in seen_scan_ids:
+            if scanseries["scanID"] in seen_scan_ids:
                 logger.warning(
-                    f"Found duplicate MS3 spectrum for MS2 spectrum with scan number {scanseries['scanID']} in {mzml_file}, known bug in ThermoRawFileParser...")
+                    f"Found duplicate MS3 spectrum for MS2 spectrum with scan number {scanseries['scanID']} in {mzml_file}, known bug in ThermoRawFileParser..."
+                )
                 continue
-            seen_scan_ids.add(scanseries['scanID'])
+            seen_scan_ids.add(scanseries["scanID"])
 
-            mz = np.array(item['m/z array'])
-            intensity = np.array(item['intensity array'])
+            mz = np.array(item["m/z array"])
+            intensity = np.array(item["intensity array"])
             for c, (low, upp) in enumerate(zip(tmt_lower, tmt_upper)):
                 start_idx = int(np.searchsorted(mz, low))
                 end_idx = int(np.searchsorted(mz, upp))
-                scanseries[f'raw_TMT{c + 1}'] = intensity[start_idx:end_idx].sum()
-            fileframe = pd.concat([fileframe, scanseries.to_frame().T], ignore_index=True)
-    fileframe['raw_file'] = mzml_file.name
+                scanseries[f"raw_TMT{c + 1}"] = intensity[start_idx:end_idx].sum()
+            fileframe = pd.concat(
+                [fileframe, scanseries.to_frame().T], ignore_index=True
+            )
+    fileframe["raw_file"] = mzml_file.name
 
     fileframe = fileframe.astype(convert_dict)
 
     # TMT correction
-    logger.info('Extraction done, correcting TMT reporters for ' + mzml_file.name)
-    fileframe[tmt_corrected_columns] = pd.DataFrame(fileframe[tmt_raw_columns].apply(
-        lambda tmt: np.linalg.lstsq(correction_normalized, tmt, rcond=None)[0].round(2), axis=1).tolist(),
-                                                    columns=tmt_corrected_columns,
-                                                    index=fileframe[tmt_corrected_columns].index)
+    logger.info("Extraction done, correcting TMT reporters for " + mzml_file.name)
+    fileframe[tmt_corrected_columns] = pd.DataFrame(
+        fileframe[tmt_raw_columns]
+        .apply(
+            lambda tmt: np.linalg.lstsq(correction_normalized, tmt, rcond=None)[
+                0
+            ].round(2),
+            axis=1,
+        )
+        .tolist(),
+        columns=tmt_corrected_columns,
+        index=fileframe[tmt_corrected_columns].index,
+    )
     # PANDAS WHERE! This retains the checked value if the condition is met, and replaces it where it is not met!
-    fileframe[tmt_corrected_columns] = fileframe[tmt_corrected_columns].where(fileframe[tmt_corrected_columns] > 10, 0)
-    fileframe.to_csv(output_file, sep='\t', index=False)
+    fileframe[tmt_corrected_columns] = fileframe[tmt_corrected_columns].where(
+        fileframe[tmt_corrected_columns] > 10, 0
+    )
+    fileframe.to_csv(output_file, sep="\t", index=False)
 
 
 def get_extracted_tmt_file_name(output_path: str, mzml_file: Path):
-    return f'{output_path}/ext_{mzml_file.name}.txt'
+    return f"{output_path}/ext_{mzml_file.name}.txt"
 
 
 def read_extracted_tmt_file(extracted_tmt_file: Path, plex: int):
     columns = {
         "raw_file": "object",
         "scanID": "int32",
         **{f"raw_TMT{i}": "float32" for i in range(1, plex + 1)},
@@ -168,38 +232,59 @@
         extracted_tmt_file,
         sep="\t",
         usecols=columns.keys(),
         dtype=columns,
     )
 
 
-def assemble_corrected_tmt_table(mzml_files: List[Path], extracted_folder: Path, plex: int):
-    logger.info('Assembling corrected reporter ion tables')
+def assemble_corrected_tmt_table(
+    mzml_files: List[Path], extracted_folder: Path, plex: int
+):
+    logger.info("Assembling corrected reporter ion tables")
 
     extracted_tmt_files = [
-        get_extracted_tmt_file_name(extracted_folder, mzml_file) 
+        get_extracted_tmt_file_name(extracted_folder, mzml_file)
         for mzml_file in mzml_files
     ]
-    corrected_tmt = utils.process_and_concat(extracted_tmt_files, read_extracted_tmt_file, plex=plex)
-    
+    corrected_tmt = utils.process_and_concat(
+        extracted_tmt_files, read_extracted_tmt_file, plex=plex
+    )
+
     corrected_tmt = corrected_tmt.reset_index(drop=True)
-    corrected_tmt = corrected_tmt.rename(columns={
-        'raw_file': 'Raw file',
-        **{f'raw_TMT{i}': f'Reporter intensity {i}' for i in range(1, plex + 1)},
-        **{f'corr_TMT{i}': f'Reporter intensity corrected {i}' for i in range(1, plex + 1)}
-    })
-    corrected_tmt['Raw file'] = corrected_tmt['Raw file'].str.replace(r'.mzML$', '', regex=True)
+    corrected_tmt = corrected_tmt.rename(
+        columns={
+            "raw_file": "Raw file",
+            **{f"raw_TMT{i}": f"Reporter intensity {i}" for i in range(1, plex + 1)},
+            **{
+                f"corr_TMT{i}": f"Reporter intensity corrected {i}"
+                for i in range(1, plex + 1)
+            },
+        }
+    )
+    corrected_tmt["Raw file"] = corrected_tmt["Raw file"].str.replace(
+        r".mzML$", "", regex=True
+    )
     return corrected_tmt
 
 
-def merge_with_corrected_tmt(msmsscans_df, corrected_tmt):
-    logger.info('Merging corrected reporter ion tables into msmsScans.txt')
-    return pd.merge(left=msmsscans_df, right=corrected_tmt, on=['Raw file', 'scanID'], how='left',
-                    validate='one_to_one')
+def merge_with_corrected_tmt(msmsscans_df: pd.DataFrame, corrected_tmt: pd.DataFrame):
+    logger.info("Merging corrected reporter ion tables into msmsScans.txt")
+    return pd.merge(
+        left=msmsscans_df,
+        right=corrected_tmt,
+        on=["Raw file", "scanID"],
+        how="left",
+        validate="one_to_one",
+    )
 
 
 if __name__ == "__main__":
     input_files_arg = Path(sys.argv[1])
-    extraction_level_arg = int(sys.argv[2])
+    extraction_level_arg = sys.argv[2]
     output_path_arg = Path(sys.argv[3])
 
-    extract_tmt_reporters([input_files_arg], output_path_arg, extraction_level=extraction_level_arg, plex=11)
+    extract_tmt_reporters(
+        [input_files_arg],
+        output_path_arg,
+        extraction_level=extraction_level_arg,
+        plex=11,
+    )
```

## Comparing `simsi_transfer-0.6.0.dist-info/LICENSE` & `simsi_transfer-0.6.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `simsi_transfer-0.6.0.dist-info/METADATA` & `simsi_transfer-0.6.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: simsi-transfer
-Version: 0.6.0
+Version: 0.6.1
 Summary: Software-assisted reduction of missing values in phosphoproteomics and proteomics isobaric labeling data using MS2 spectrum clustering
 Home-page: https://github.com/kusterlab/SIMSI-Transfer
 License: Apache-2.0
 Keywords: mass spectrometry,missing values,clustering,proteomics,quantification
 Author: Firas Hamood
 Author-email: firas.hamood@tum.de
 Requires-Python: >=3.8,<4.0
```

## Comparing `simsi_transfer-0.6.0.dist-info/RECORD` & `simsi_transfer-0.6.1.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 LICENSE,sha256=qnTo57BuJqeIWA5eC3KoagS1se_Kr85R2qcQn_G1Ejw,11355
 THERMO_LICENSE,sha256=WA2rCjRynvTI4CE6Qomk59gHzeAn6Ru3VIzvlEog8i8,6510
 simsi_transfer/__init__.py,sha256=DdjELd9dN8yIPIWW_rb7LhWXmm6Sr1IsTt6Q8pvcDcA,1576
 simsi_transfer/__main__.py,sha256=2ZUKtiq-OhHkSdWgWyYdi3BBxK1EygJAL5JGQmMkEqs,166
-simsi_transfer/command_line_interface.py,sha256=YF0qf5I4VxsWP0cwreQLUWD0uPMTR8mUgfxpMaxOKnE,7303
+simsi_transfer/command_line_interface.py,sha256=NpqLvEGHuxG-elKkDTLHMP1EpRCSnJdV21Mm8ZlgNTs,7554
 simsi_transfer/evidence.py,sha256=m92gTaht7BthgO04uylIJALIcFTrhnc8Gtn7Xu5gL04,15113
-simsi_transfer/main.py,sha256=0JkqaU3_Gpr9nalEzTRESzhwg2kcO84XU4NFGn5Q0tM,7647
+simsi_transfer/main.py,sha256=roiDM2ERK9WR--D7bqNvxFHipaBvLn--DMlzPlaaFAs,7734
 simsi_transfer/maracluster.py,sha256=bZZuERZoIiDWjcuQ2yjikfH7rfS4TY2gP97S8gOfOlk,3101
 simsi_transfer/maxquant.py,sha256=9gcNTMvSBoAbmkJg4It9nFUCYC_9BFM4c96mHCCEB44,5847
 simsi_transfer/merging_functions.py,sha256=vQ0sQ3xO1mK_nNwxj1IVMnxTTcwPBLKBuDZo869xn-s,2090
 simsi_transfer/simsi_output.py,sha256=kT3hriUdpTx7ZAhiaq-uMzgZrPUIHZqkz-l_o1PRu0Q,4374
 simsi_transfer/thermo_raw.py,sha256=uM-Xe0n6_oO63lBTJiFNOzeM48oh1veURdTeWZMaFG8,5412
-simsi_transfer/tmt_processing.py,sha256=HCZJgnqNxECav-QrShWEXDJtg7rJLNgmuxPt4tz4pyA,9681
+simsi_transfer/tmt_processing.py,sha256=fIoEiBaceHXuc0kenEo_p0bYOXUezuzvNKA40JbsXek,10424
 simsi_transfer/transfer.py,sha256=6D8N6qtu7ywS0tfHrUBxPbJsClbnrlnUJZnjQAKtE4M,11097
 simsi_transfer/utils/ThermoRawFileParser/AWS.Logger.Core.dll,sha256=UWVWc3OFWawu9hJED4ActzcFPUTtSYgmlQXX7VUw8Z0,33200
 simsi_transfer/utils/ThermoRawFileParser/AWS.Logger.Core.pdb,sha256=iicUoJb_LKdYIX4SjjdvgH70NCgIQqUIQeK_wn802CE,5384
 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.CloudWatchLogs.dll,sha256=GykB-14xYqGSuJamf9_pIeBomfNwgRDxi4VK_s_ZP4c,136192
 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.CloudWatchLogs.pdb,sha256=vw1xNlDz4mN7idXVuh4OROXf-fDghA2Du4FZS1p6Kbg,73260
 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.Core.dll,sha256=jz5ds9Lr6rEZHkXRJWdceK5gryVi5LJlkTfL6SPiBW4,904704
 simsi_transfer/utils/ThermoRawFileParser/AWSSDK.Core.pdb,sha256=Ns9BEHc2Ndr8CfQZgFymuIeNjC5k1eD8HDnfafiEnbM,245016
@@ -55,11 +55,11 @@
 simsi_transfer/utils/ThermoRawFileParser/nunit.framework.dll,sha256=ldjehxmJc2I1KI2AOrd8W8mFPkuOBFhWs7qEizTSSGQ,480768
 simsi_transfer/utils/ThermoRawFileParser/testcentric.engine.metadata.dll,sha256=i6SkMDBxRUmOxnssGtVArlLn_2g8iuO6E4iyvYQHssE,177152
 simsi_transfer/utils/ThermoRawFileParser/zlib.net.dll,sha256=QWe1mfNq8HgbgVY70heabTXaoxRfC2-ZrG2e4YlLUWo,69632
 simsi_transfer/utils/maracluster/linux64/maracluster,sha256=NaQc11-LgFTUiimg2xLLAGSmdZi7XD9blJxLY9riTeA,7096664
 simsi_transfer/utils/maracluster/win64/maracluster.exe,sha256=Yr_Nh4ux1QwEXPwUcXOagCohZU2FJaBTY37XfMhnWmQ,5667840
 simsi_transfer/utils/subprocess_with_logger.py,sha256=ADz7T6pbrsoU2CIKiSD1Jw3YwInKxrr08A-R-Kvv0cM,840
 simsi_transfer/utils/utils.py,sha256=pkcFAUStAIFbhVs8hB4TMOWaa_4CnCuomoh9Uq8Y4Qw,3366
-simsi_transfer-0.6.0.dist-info/LICENSE,sha256=qnTo57BuJqeIWA5eC3KoagS1se_Kr85R2qcQn_G1Ejw,11355
-simsi_transfer-0.6.0.dist-info/WHEEL,sha256=DA86_h4QwwzGeRoz62o1svYt5kGEXpoUTuTtwzoTb30,83
-simsi_transfer-0.6.0.dist-info/METADATA,sha256=9iiUEs_FSo7hcAQLQIGJyP0hkgu60XYWTDRSjE_Zneg,4731
-simsi_transfer-0.6.0.dist-info/RECORD,,
+simsi_transfer-0.6.1.dist-info/LICENSE,sha256=qnTo57BuJqeIWA5eC3KoagS1se_Kr85R2qcQn_G1Ejw,11355
+simsi_transfer-0.6.1.dist-info/WHEEL,sha256=DA86_h4QwwzGeRoz62o1svYt5kGEXpoUTuTtwzoTb30,83
+simsi_transfer-0.6.1.dist-info/METADATA,sha256=zWvtPL50iwCrwGa0JDUZA5LQSgLK1f4hHg7Ajeug_O8,4731
+simsi_transfer-0.6.1.dist-info/RECORD,,
```

