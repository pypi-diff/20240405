# Comparing `tmp/asktable-0.13.5-py3-none-any.whl.zip` & `tmp/asktable-0.13.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,34 @@
-Zip file size: 27958 bytes, number of entries: 32
+Zip file size: 29098 bytes, number of entries: 32
 -rw-r--r--  2.0 unx       86 b- defN 24-Apr-02 15:29 asktable/__init__.py
 -rw-r--r--  2.0 unx     2180 b- defN 24-Apr-01 06:16 asktable/api.py
 -rw-r--r--  2.0 unx     1667 b- defN 24-Apr-01 06:16 asktable/cli.py
--rw-r--r--  2.0 unx     1220 b- defN 24-Apr-01 06:16 asktable/client.py
+-rw-r--r--  2.0 unx      887 b- defN 24-Apr-05 04:16 asktable/client.py
 -rw-r--r--  2.0 unx       41 b- defN 24-Apr-01 06:16 asktable/exceptions.py
 -rw-r--r--  2.0 unx      875 b- defN 24-Apr-01 06:44 asktable/log.py
 -rw-r--r--  2.0 unx     1492 b- defN 24-Apr-01 06:16 asktable/upload.py
 -rw-r--r--  2.0 unx      152 b- defN 24-Apr-01 06:16 asktable/models/__init__.py
 -rw-r--r--  2.0 unx     2588 b- defN 24-Apr-01 06:16 asktable/models/base.py
 -rw-r--r--  2.0 unx     2769 b- defN 24-Apr-01 06:16 asktable/models/chat.py
--rw-r--r--  2.0 unx     7670 b- defN 24-Apr-02 15:39 asktable/models/datasource.py
+-rw-r--r--  2.0 unx    10178 b- defN 24-Apr-05 03:12 asktable/models/datasource.py
 -rw-r--r--  2.0 unx     1264 b- defN 24-Apr-01 06:16 asktable/models/message.py
 -rw-r--r--  2.0 unx      878 b- defN 24-Apr-01 06:16 asktable/models/run.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Dec-08 14:27 atcommon/__init__.py
--rw-r--r--  2.0 unx     3352 b- defN 24-Mar-20 07:26 atcommon/tools.py
--rw-r--r--  2.0 unx       16 b- defN 24-Apr-02 15:39 atcommon/version.py
+-rw-r--r--  2.0 unx     3296 b- defN 24-Apr-04 03:57 atcommon/tools.py
+-rw-r--r--  2.0 unx       16 b- defN 24-Apr-05 04:31 atcommon/version.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-31 13:33 atcommon/exceptions/__init__.py
 -rw-r--r--  2.0 unx     1282 b- defN 24-Apr-02 15:21 atcommon/exceptions/client.py
 -rw-r--r--  2.0 unx       95 b- defN 24-Apr-01 06:16 atcommon/exceptions/server.py
 -rw-r--r--  2.0 unx     2089 b- defN 24-Apr-02 14:43 atcommon/exceptions/server_base.py
 -rw-r--r--  2.0 unx      589 b- defN 24-Apr-01 06:16 atcommon/exceptions/server_plugins.py
 -rw-r--r--  2.0 unx      466 b- defN 24-Apr-01 06:16 atcommon/models/__init__.py
 -rw-r--r--  2.0 unx      731 b- defN 24-Jan-20 03:26 atcommon/models/base.py
 -rw-r--r--  2.0 unx     2344 b- defN 24-Apr-01 06:16 atcommon/models/bi.py
 -rw-r--r--  2.0 unx     1069 b- defN 24-Apr-01 06:16 atcommon/models/chat.py
--rw-r--r--  2.0 unx     1535 b- defN 24-Apr-02 14:59 atcommon/models/datasource.py
--rw-r--r--  2.0 unx    23566 b- defN 24-Apr-02 12:29 atcommon/models/meta.py
--rw-r--r--  2.0 unx     8309 b- defN 24-Apr-02 15:39 asktable-0.13.5.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-02 15:39 asktable-0.13.5.dist-info/WHEEL
--rw-r--r--  2.0 unx       47 b- defN 24-Apr-02 15:39 asktable-0.13.5.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       18 b- defN 24-Apr-02 15:39 asktable-0.13.5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2558 b- defN 24-Apr-02 15:39 asktable-0.13.5.dist-info/RECORD
-32 files, 71040 bytes uncompressed, 23878 bytes compressed:  66.4%
+-rw-r--r--  2.0 unx     1424 b- defN 24-Apr-03 14:46 atcommon/models/datasource.py
+-rw-r--r--  2.0 unx    25728 b- defN 24-Apr-04 14:45 atcommon/models/meta.py
+-rw-r--r--  2.0 unx     9875 b- defN 24-Apr-05 04:31 asktable-0.13.6.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-05 04:31 asktable-0.13.6.dist-info/WHEEL
+-rw-r--r--  2.0 unx       47 b- defN 24-Apr-05 04:31 asktable-0.13.6.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       18 b- defN 24-Apr-05 04:31 asktable-0.13.6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2558 b- defN 24-Apr-05 04:31 asktable-0.13.6.dist-info/RECORD
+32 files, 76776 bytes uncompressed, 25018 bytes compressed:  67.4%
```

## zipnote {}

```diff
@@ -75,23 +75,23 @@
 
 Filename: atcommon/models/datasource.py
 Comment: 
 
 Filename: atcommon/models/meta.py
 Comment: 
 
-Filename: asktable-0.13.5.dist-info/METADATA
+Filename: asktable-0.13.6.dist-info/METADATA
 Comment: 
 
-Filename: asktable-0.13.5.dist-info/WHEEL
+Filename: asktable-0.13.6.dist-info/WHEEL
 Comment: 
 
-Filename: asktable-0.13.5.dist-info/entry_points.txt
+Filename: asktable-0.13.6.dist-info/entry_points.txt
 Comment: 
 
-Filename: asktable-0.13.5.dist-info/top_level.txt
+Filename: asktable-0.13.6.dist-info/top_level.txt
 Comment: 
 
-Filename: asktable-0.13.5.dist-info/RECORD
+Filename: asktable-0.13.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## asktable/client.py

```diff
@@ -26,16 +26,7 @@
     def chats(self):
         return ChatList(api=self.api, endpoint="/chats")
 
     @property
     def token_id(self):
         return self.api.send(endpoint="/account/token", method="GET")
 
-    def test_clear_all(self):
-        return self.api.send(endpoint="/tests/clear_data/all", method="POST")
-
-    def test_clear_brain(self):
-        return self.api.send(endpoint="/tests/clear_data/brain", method="POST")
-
-    def test_clear_chat(self):
-        return self.api.send(endpoint="/tests/clear_data/chat", method="POST")
-
```

## asktable/models/datasource.py

```diff
@@ -11,19 +11,79 @@
     DataSourceMetaProcessTimeout
 )
 from asktable.upload import upload_to_oss
 
 
 class MetaDataClientModel(MetaData):
 
-    def sync(self):
-        new_meta_dict = self.api.send(endpoint=f"/datasources/{self.datasource_id}/meta", method="POST")
+    @property
+    def status(self):
+        ds = self.api.send(endpoint=f"/datasources/{self.datasource_id}", method="GET")
+        return ds['meta_status']
+
+    def get(self):
+        new_meta_dict = self.api.send(endpoint=f"/datasources/{self.datasource_id}/meta", method="GET")
         self.update_from_dict(new_meta_dict)
         return self
 
+    def delete(self):
+        self.api.send(endpoint=f"/datasources/{self.datasource_id}/meta", method="DELETE")
+        return True
+
+    def update_async(self):
+        self.api.send(endpoint=f"/datasources/{self.datasource_id}/meta", method="POST")
+        return True
+
+    def update(self, timeout=600):
+        """
+        同步元数据并轮询其状态，直到超时、成功或失败。
+
+        参数:
+            timeout (int): 超时时间，单位为秒。默认值为 600 秒。
+
+        返回:
+            MetaDataClientModel: 更新后的元数据实例。
+
+        异常:
+            DataSourceMetaProcessError: 如果元数据更新失败。
+            DataSourceMetaProcessTimeout: 如果元数据更新超时。
+        """
+        # 发起同步请求
+        self.update_async()
+
+        start_time = time.time()
+
+        # 初始化斐波那契数列间隔
+        fib_a, fib_b = 1, 1
+
+        while True:
+            # 获取数据源的最新状态
+            status = self.status
+
+            if status == 'success':
+                log.info(f"Data Source {self.datasource_id} Meta 更新成功。")
+                return self.get()
+            elif status == 'failed':
+                log.error(f"Data Source {self.datasource_id} Meta 更新失败。")
+                raise DataSourceMetaProcessError(f"DataSource Meta update failed: {self.datasource_id}")
+
+            if time.time() - start_time > timeout:
+                log.error(f"Data Source {self.datasource_id} Meta 更新超时。")
+                raise DataSourceMetaProcessTimeout(
+                    f"DataSource Meta update timed out after {timeout} seconds for {self.datasource_id}")
+
+            # 等待斐波那契数列定义的时间，或者最多10秒
+            sleep_time = min(fib_b, 10)
+            log.info(f"Data Source {self.datasource_id}：等待 {sleep_time} 秒后重新检查。当前状态: {status}")
+            time.sleep(sleep_time)
+
+            # 更新斐波那契数列的值
+            if fib_b < 10:
+                fib_a, fib_b = fib_b, fib_a + fib_b
+
 
 class DataSourceClientModel(DataSourceCore):
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.temp_chat = None  # 初始化时没有chat实例
 
@@ -132,38 +192,46 @@
         data = {
             "type": type, "name": name,
             "access_config": access_config
         }
         return self.api.send(endpoint=self.endpoint, method="POST",
                              data=data)
 
-    @convert_to_object(cls=DataSourceClientModel)
     def register(self, type, access_config, name=None, timeout=180):
         # 注册数据源并获取其ID
         ds = self.register_async(type, access_config, name)
         start_time = time.time()
 
+        # 初始化斐波那契数列间隔
+        fib_a, fib_b = 1, 1
+
         while True:
             # 获取数据源的最新状态
             ds_refreshed = self.get(id=ds.id)
 
             if ds_refreshed.meta_status == 'success':
-                log.info(f"Data Source {ds.id} Meta 更新成功。")
+                log.info(f"Data Source {ds.id}  AI Process Success.")
                 return ds_refreshed
             elif ds_refreshed.meta_status == 'failed':
-                log.error(f"Data Source {ds.id} Meta 更新失败。")
-                raise DataSourceMetaProcessError(f"DataSource Meta update failed: {ds.id}")
+                log.error(f"Data Source {ds.id}  AI Process Failed.")
+                raise DataSourceMetaProcessError(f"DataSource AI Process failed: {ds.id}")
 
             if time.time() - start_time > timeout:
-                log.error(f"Data Source {ds.id} Meta 更新超时。")
+                log.error(f"Data Source {ds.id}  AI分析超时。")
                 raise DataSourceMetaProcessTimeout(
-                    f"DataSource Meta update timed out after {timeout} seconds for {ds.id}")
+                    f"DataSource AI Process timed out after {timeout} seconds for {ds.id}")
 
-            log.info(f"Data Source {ds.id} Meta 状态为 {ds_refreshed.meta_status}，正在等待更新...")
-            time.sleep(10)  # 每10秒检查一次
+            # 等待斐波那契数列定义的时间，或者最多10秒
+            sleep_time = min(fib_b, 10)
+            log.info(f"Data Source {ds.id}: Wait for {sleep_time} seconds and check again. Current status: {ds_refreshed.meta_status}")
+            time.sleep(sleep_time)
+
+            # 更新斐波那契数列的值
+            if fib_b < 10:
+                fib_a, fib_b = fib_b, fib_a + fib_b
 
     def upload_file_to_oss(self, local_file_path):
         oss_info = self.create_upload_params()['oss']
         file_name = os.path.basename(local_file_path)
         oss_file_uri = f"{oss_info['oss_uri_prefix']}{file_name}"
         url = upload_to_oss(oss_info, local_file_path, oss_file_uri)
         return url
```

## atcommon/tools.py

```diff
@@ -76,18 +76,14 @@
 def gen_id(prefix=None):
     if prefix:
         return f"{prefix}_{gen_base62_uuid()}"
     else:
         return gen_base62_uuid()
 
 
-def gen_uuid_for_vector():
-    return str(uuid.uuid1())
-
-
 def gen_tenant_id():
     return f"t_{gen_base62_uuid()}"
 
 def gen_token_id():
     return f"token_{gen_base62_uuid()}"
 
 
@@ -102,7 +98,9 @@
 def gen_id_for_msg():
     return f"msg_{gen_base62_uuid()}"
 
 
 def gen_id_for_run():
     return f"run_{gen_base62_uuid()}"
 
+
+
```

## atcommon/version.py

```diff
@@ -1 +1 @@
-VERSION="0.13.5"
+VERSION="0.13.6"
```

## atcommon/models/datasource.py

```diff
@@ -6,18 +6,15 @@
 
     __properties_init__ = ['tenant_id', 'id', 'created_at',
                            'name', 'type', 'access_config',
                            'sample_questions', 'meta_status']
 
     def __repr__(self):
         if self.id:
-            if self.name:
-                return f"<{self.id}-{self.name} >"
-            else:
-                return f"<{self.id}-(not named)>"
+            return f"<{self.id}>"
         else:
             return f"<{self.type} (not saved)>"
 
     def __str__(self):
         return self.__repr__()
 
     @property
```

## atcommon/models/meta.py

```diff
@@ -23,27 +23,26 @@
     def db_properties(self):
         return {k: v for k, v in self.__dict__.items()
                 if k in self._save_in_db_properties}
 
     def to_dict(self):
         raise NotImplementedError
 
-    def to_dict_for_llm(self):
-        raise NotImplementedError
-
     @json_to_string
     def to_string(self):
         return self.to_dict()
 
-    @json_to_string
-    def to_string_for_llm(self):
-        return self.to_dict_for_llm()
 
-    def __str__(self):
-        return self.to_string()
+    #
+    # def to_dict_for_analysis(self):
+    #     raise NotImplementedError
+    #
+    # @json_to_string
+    # def to_string_for_analysis(self):
+    #     return self.to_dict_for_analysis()
 
 
 class DataField(DataBaseObject):
     _save_in_db_properties = ['name', 'full_name', 'origin_desc',
                               'curr_desc', 'curr_desc_stat',
                               'sample_data'
                               ]
@@ -124,14 +123,17 @@
         self.fields = []
         self.fields_dict: Dict[str, DataField] = {}
 
     def add_field(self, field: DataField):
         self.fields.append(field)
         self.fields_dict[field.name] = field
 
+    def get_field(self, field_name):
+        return self.fields_dict.get(field_name)
+
     def __repr__(self):
         return f"<DataTable(name='{self.full_name}', curr_desc='{self.curr_desc}')>"
 
     def to_dict(self, level='field', includes=(), excludes=()):
         _d = {}
         for i in self._in_dict_properties:
             if i in excludes or (includes and i not in includes):
@@ -139,20 +141,22 @@
             _d[i] = getattr(self, i)
 
         if level == 'field':
             _d['fields'] = [f.to_dict(includes=includes, excludes=excludes)
                             for f in self.fields]
         return _d
 
-    def to_dict_for_llm(self):
-        # 如果有任何一个字段的 curr_desc 为空，则需要AI生成
-        # table 中去掉一些不需要的字段，减少token数量
-        # 保留：name, curr_desc
-        need_infer = any(not f.curr_desc for f in self.fields)
-        return self.to_dict(includes=('name', 'curr_desc')) if need_infer else {}
+    # def to_dict_for_analysis(self):
+    #     # 如果有任何一个字段的 curr_desc 为空，则需要AI生成
+    #     # table 中去掉一些不需要的字段，减少token数量
+    #     # 保留：name, curr_desc
+    #     need_analyse = any(not f.curr_desc for f in self.fields)
+    #     if not self.curr_desc:
+    #         need_analyse = True
+    #     return self.to_dict(includes=('name', 'curr_desc')) if need_analyse else {}
 
 
 class DataSchema(DataBaseObject):
     _save_in_db_properties = ['name', 'origin_desc', 'curr_desc',
                               'curr_desc_stat', 'custom_configs']
     _in_dict_properties = _save_in_db_properties
 
@@ -172,14 +176,17 @@
         self.tables = []
         self.tables_dict: Dict[str, DataTable] = {}
 
     def add_table(self, table: DataTable):
         self.tables.append(table)
         self.tables_dict[table.name] = table
 
+    def get_table(self, table_name):
+        return self.tables_dict.get(table_name)
+
     @property
     def custom_configs_dict(self):
         if not self.custom_configs:
             return {}
         if isinstance(self.custom_configs, dict):
             return self.custom_configs
         elif isinstance(self.custom_configs, str):
@@ -207,39 +214,48 @@
             _d[i] = getattr(self, i)
 
         if level in ('table', 'field'):
             _d['tables'] = [t.to_dict(level, includes=includes, excludes=excludes)
                             for t in self.tables]
         return _d
 
-    def to_dict_for_llm(self):
-        # if table.to_dict_for_llm() is empty, it will be ignored
-        need_infer_tables = [table.to_dict_for_llm() for table in self.tables
-                             if table.to_dict_for_llm()]
-        if need_infer_tables or (not self.curr_desc):
-            return {
-                'name': self.name,
-                'curr_desc': self.curr_desc,
-                'tables': need_infer_tables
-            }
-        else:
-            return {}
+    # def to_dict_for_analysis(self):
+    #     # if table.to_dict_for_analyse() is empty, it will be ignored
+    #     need_infer_tables = [table.to_dict_for_analysis() for table in self.tables
+    #                          if table.to_dict_for_analysis()]
+    #     if need_infer_tables or (not self.curr_desc):
+    #         return {
+    #             'name': self.name,
+    #             'curr_desc': self.curr_desc,
+    #             'tables': need_infer_tables
+    #         }
+    #     else:
+    #         return {}
 
 
 class MetaData(DataBaseObject):
     def __init__(self, name, datasource_id=''):
         self.name = name
         self.datasource_id = datasource_id
         self.schemas = []
         self.schemas_dict: Dict[str, DataSchema] = {}
 
     def add_schema(self, schema: DataSchema):
         self.schemas.append(schema)
         self.schemas_dict[schema.name] = schema
 
+    def get_schema(self, schema_name):
+        return self.schemas_dict.get(schema_name)
+
+    def filter_for_analysis(self) -> "MetaData":
+        """
+        返回一个新的 MetaData 对象，其中只包含需要分析的字段。
+        """
+        return self.load_from_analysed_simple_dict(self.to_simple_dict_for_analysis())
+
     @classmethod
     def load_from_dict(cls, data: dict):
         """
         Load metadata from a dictionary structure.
         The expected format is:
         {
             'name': 'metadata_name',
@@ -403,40 +419,36 @@
         for field_name, related_field_name in temp_related_fields.items():
             field = self.get_field_by_full_name(field_name)
             related_field = self.get_field_by_full_name(related_field_name)
             if field and related_field:
                 # 设置字段关联
                 field.set_related_field(related_field)
 
-    def __repr__(self):
-        return f"{self.datasource_id} {self.summary} \n\n{self.to_table()}"
 
     def to_dict(self, level='field', includes=(), excludes=()):
         # level: field, table, schema
         # includes 默认取全部
         # excludes 优先级高于includes
         return {
             'name': self.name,
             'datasource_id': self.datasource_id,
             'schemas': [schema.to_dict(level, includes=includes, excludes=excludes)
                         for schema in self.schemas]
         }
 
-    def to_dict_for_agent(self):
+    def to_dict_for_prompt(self):
         result = {
             'name': self.name,
             'datasource_id': self.datasource_id,
             'schemas': [schema.to_dict(
                 level='field',
                 includes=(
                     'name',
                     'full_name',
                     'curr_desc',
-                    'related_field',
-                    'related_field_precision_rate',
                 )) for schema in self.schemas]
         }
 
         # 修改fields的结构，减少token数量
         for schema in result['schemas']:
             for table in schema.get('tables', []):
                 for field in table.get('fields', []):
@@ -447,74 +459,112 @@
                         field.pop('related_field_precision_rate', None)
 
                     # 更名，更直接，更少的字符
                     field['desc'] = field.pop('curr_desc', None)
 
         return result
 
-    def to_markdown_for_agent(self):
-        return dict_to_markdown(self.to_dict_for_agent(), table_format_keys=('fields',))
+    def to_markdown_for_prompt(self):
+        return dict_to_markdown(self.to_dict_for_prompt(), table_format_keys=('fields',))
 
-    def to_dict_for_llm(self):
+    # def to_dict_for_analysis(self):
+    #     return {
+    #         'name': self.name,
+    #         'datasource_id': self.datasource_id,
+    #         'schemas': [schema.to_dict_for_analysis() for schema
+    #                     in self.schemas if schema.to_dict_for_analysis()]
+    #     }
+    #
+    # def to_markdown_for_analysis(self):
+    #     return dict_to_markdown(self.to_dict_for_analysis(), table_format_keys=('fields',))
+
+    def to_simple_dict_for_analysis(self) -> dict:
+        schemas = []
+        for s in self.schemas:
+            tables = []
+            for t in s.tables:
+                if t.curr_desc and all([f.curr_desc for f in t.fields]):
+                    continue
+                fields = []
+                for f in t.fields:
+                    if f.curr_desc and f.curr_desc_stat in ('human', 'origin'):
+                        fields.append((f.name, f.curr_desc))
+                    else:
+                        fields.append((f.name, ''))
+                tables.append({
+                    'name': t.name,
+                    'fields': fields
+                })
+            schemas.append({
+                'name': s.name,
+                'tables': tables
+            })
         return {
             'name': self.name,
             'datasource_id': self.datasource_id,
-            'schemas': [schema.to_dict_for_llm() for schema
-                        in self.schemas if schema.to_dict_for_llm()]
+            'schemas': schemas,
         }
 
-    @property
-    def summary(self):
+    @classmethod
+    def load_from_analysed_simple_dict(cls, data: dict) -> "MetaData":
         """
-        Summarize the metadata。只有Schema和Table给出前10条 item，Field不给出。
-        Like this:
-            共有：1 Schema (public), 1 Table (users), 2 Fields.
-        """
-        summary = f"共有："
-
-        # Summarizing schemas
-        schema_count = len(self.schemas)
-        if schema_count <= 1:
-            summary += f"{schema_count} Schema ("
-        else:
-            summary += f"{schema_count} Schemas ("
-        for i, schema in enumerate(self.schemas[:10]):
-            summary += schema.name
-            if i < min(9, schema_count - 1):  # Add comma if not the last item
-                summary += ","
-        if schema_count > 10:
-            summary += "..."
-        summary += "), "
-
-        # Summarizing tables and fields
-        total_tables = 0
-        total_fields = 0
-        table_names = []
+        data =
+        {
+            "name": [name],
+            "datasource_id": [datasource_id],
+            "schemas": [
+                {
+                    "name": [schema_name],
+                    "curr_desc": [schema_curr_desc],
+                    "tables": [
+                        {
+                            "name": [table_name],
+                            "curr_desc": [table_curr_desc],
+                            "fields": [
+                                ["filed_name_1", "filed_curr_desc_1"],
+                                ["filed_name_2", "filed_curr_desc_2"],
+                            ]
+                        },
+                    ]
+                }
+            ]
+        }
+        """
+        metadata = cls(data.get('name', ''), data.get('datasource_id', ''))
 
-        for schema in self.schemas:
-            total_tables += len(schema.tables)
-            for table in schema.tables:
-                total_fields += len(table.fields)
-                if len(table_names) <= 5:
-                    table_names.append(table.full_name)
-
-        # Formatting tables list
-        table_list = ",".join(table_names)
-        if len(table_names) > 5:
-            table_list += "..."
+        # 遍历分析后的数据中的 schema
+        for schema_data in data.get('schemas', []):
+            schema = DataSchema(
+                name=schema_data.get('name', ''),
+                metadata=metadata,
+                curr_desc=schema_data.get('curr_desc', ''),
+            )
 
-        if total_tables <= 1:
-            summary += f"{total_tables} Table ({table_list}), "
-        else:
-            summary += f"{total_tables} Tables ({table_list}), "
-        if total_fields <= 1:
-            summary += f"{total_fields} Field."
-        else:
-            summary += f"{total_fields} Fields."
-        return summary
+            # 遍历 schema 中的 table
+            for table_data in schema_data.get('tables', []):
+                table = DataTable(
+                    name=table_data.get('name', ''),
+                    schema=schema,
+                    curr_desc=table_data.get('curr_desc', ''),
+                )
+
+                # 遍历 table 中的 fields
+                for field_data in table_data.get('fields', []):
+                    field = DataField(
+                        name=field_data[0],  # field 名称
+                        table=table,
+                        curr_desc=field_data[1],  # field 描述
+                    )
+                    table.add_field(field)
+
+                schema.add_table(table)
+
+            metadata.add_schema(schema)
+
+        return metadata
 
     def get_field_by_full_name(self, full_name):
         """
         根据字段的完整名称来查找字段对象。
         完整名称格式为 "schema_name.table_name.field_name"
         """
         schema_name, table_name, field_name = full_name.split('.')
@@ -523,47 +573,59 @@
                 for table in schema.tables:
                     if table.name == table_name:
                         for field in table.fields:
                             if field.name == field_name:
                                 return field
         return None
 
-    def to_table(self):
+    @property
+    def overview(self):
+        return f"<{self.datasource_id}>[{self.schema_count}S|{self.table_count}T|{self.field_count}F]"
+
+    def __str__(self):
+        return self.__repr__()
+
+    def __repr__(self):
         output = ""
         for schema in self.schemas:
             schema_data = []  # Prepare data for the current schema
             schema_desc = schema.name
             schema_desc += f" ({schema.curr_desc})" if schema.curr_desc else ""
             schema_desc += "\n" + "-" * 30
 
             for table in schema.tables:
                 table_name = table.name
                 table_desc = table.curr_desc if table.curr_desc else ""
-                known_f = len([f for f in table.fields if f.curr_desc])
+                analysed_f = len([f for f in table.fields if f.curr_desc])
                 all_f = len(table.fields)
-                fields = f"{known_f}/{all_f}"
+                fields = f"{analysed_f}/{all_f}"
                 schema_data.append({
                     'Table Name': table_name,
                     'Table Desc': table_desc,
-                    'Fields(known/all)': fields
+                    'Fields(analysed/all)': fields
                 })
 
             output += f"{schema_desc}\n" + tabulate(schema_data, headers="keys", tablefmt="plain") + "\n\n"
 
         return output.strip()
 
+
+    @property
+    def schema_count(self):
+        return len(self.schemas)
+
     @property
     def table_count(self):
         return sum([len(schema.tables) for schema in self.schemas])
 
     @property
     def field_count(self):
         return sum([len(table.fields) for schema in self.schemas for table in schema.tables])
 
-    def split_into_chunks(self, max_tables_per_chunk=1):
+    def split_into_chunks(self, max_tables_per_chunk=5) -> List["MetaData"]:
         """
         Split the MetaData into smaller MetaData chunks, each with a maximum of 10 tables.
         """
         chunks = []
         current_chunk = None
         table_count = 0
 
@@ -592,11 +654,10 @@
 
                 # 请确保在复制前后，更新 cloned_table 的 schema 属性
                 cloned_table.schema = cloned_schema
 
                 # 现在可以安全地添加 cloned_table 到新 schema 中
                 cloned_schema.add_table(cloned_table)
 
-
                 table_count += 1
 
         return chunks
```

## Comparing `asktable-0.13.5.dist-info/METADATA` & `asktable-0.13.6.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: asktable
-Version: 0.13.5
+Version: 0.13.6
 Summary: AskTable SDK
 Home-page: https://asktable.com/
 Author: lele
 Description-Content-Type: text/markdown
 Requires-Dist: requests
 Requires-Dist: tabulate
 Requires-Dist: pybase62
@@ -68,14 +68,20 @@
             "host": "localhost",
             "port": 3306,
             "user": "root",
             "password": "",
             "db": "test"  # 可选
         }
     )
+    Data Source ds_2MSqC5EUFawpK0nVOg8MXN: Wait for 1 seconds and check again. Current status: processing
+    Data Source ds_2MSqC5EUFawpK0nVOg8MXN: Wait for 2 seconds and check again. Current status: processing
+    Data Source ds_2MSqC5EUFawpK0nVOg8MXN: Wait for 3 seconds and check again. Current status: processing
+    Data Source ds_2MSqC5EUFawpK0nVOg8MXN: Wait for 5 seconds and check again. Current status: processing
+    Data Source ds_2MSqC5EUFawpK0nVOg8MXN: Wait for 8 seconds and check again. Current status: processing
+    Data Source ds_2MSqC5EUFawpK0nVOg8MXN  AI Process Success.
     ```
 2. 注册一个可下载的文件(Excel或CSV)
     ```python
     at.datasources.register(
         type="csv",
         access_config={
             "location_url": "https://example.com/path/to/myfile.csv",
@@ -124,90 +130,108 @@
     In [57]: at.datasources.get(name='杭州房产信息')
     Out[57]: 
     [<ds_25vqFPq3wAgzcCVGoygBOO: 杭州房产信息 >,
      <ds_2eGWHUvUbWN0AAbke0aKIb: 杭州房产信息 >,
      <ds_2TDFez0l0qiSwFLsGvRKvV: 杭州房产信息 >]
     ```
 
-### 管理某个数据源
+### 管理数据源
 
-1. 查看 AskTable 中的Meta（即AI使用的Meta，在 AskTable 中所保存的Meta）
+1. 查看Meta（在AskTable中管理的Meta）
     ```python
-    In [58]: ds = at.datasources.latest
-    
-    In [59]: ds.meta
-    Out[59]: 
-    ds_2TDFez0l0qiSwFLsGvRKvV 共有：1 Schema (杭州房产信息), 1 Table (杭州房产信息.杭州房产信息), 19 Fields. 
-    
-    Schemas       Tables        Fields(known/all)
-    杭州房产信息  杭州房产信息  19/19
+    In [15]: at.datasources.latest.meta
+    Out[15]: 
+    test (测试环境数据库)
+    ------------------------------
+    Table Name       Table Desc            Fields(analysed/all)
+    alembic_version  数据库迁移版本控制表  1/1
+    chats            聊天记录表            4/4
+    users            用户信息表            2/2
     ```
    
-2. 查看运行时的Meta（即原文件或者原数据库中最新的Meta）
+2. 查看 RuntimeMeta（运行时，即原文件或者原数据库中最新的Meta）
     ```python
-    In [61]: ds.meta_runtime
-    Out[61]: 
-    ds_2TDFez0l0qiSwFLsGvRKvV 共有：1 Schema (杭州房产信息), 1 Table (杭州房产信息.杭州房产信息), 19 Fields. 
-    
-    Schemas       Tables        Fields(known/all)
-    杭州房产信息  杭州房产信息  0/19
+    In [16]: at.datasources.latest.meta_runtime
+    Out[16]: 
+    test
+    ------------------------------
+    Table Name       Table Desc    Fields(analysed/all)
+    alembic_version                0/1
+    chats                          1/4
+    users                          1/2
+    ```
+3. 更新Meta（即将 RuntimeMeta 更新到 AskTable 中，并自动分析和识别含义）
+    ```python
+    In [17]: at.datasources.latest.meta.update()
+    Data Source ds_39dJslzX5G2TDofefHJ0t9 Meta 状态为 processing，正在等待完成...
+    等待 1 秒后重新检查。当前状态: processing
+    Data Source ds_39dJslzX5G2TDofefHJ0t9 Meta 更新成功。
+    Out[17]: 
+    test (测试环境数据库)
+    ------------------------------
+    Table Name       Table Desc            Fields(analysed/all)
+    alembic_version  数据库迁移版本控制表  1/1
+    chats            聊天记录表            4/4
+    users            用户信息表            2/2
     ```
-3. 同步Meta（即将运行时的Meta同步到 AskTable 中的Meta）
+
+4. 查看 Meta 状态
     ```python
-    In [60]: ds.meta.sync()
-    Out[60]: True
+    In [21]: at.datasources.latest.meta.status
+    Out[21]: 'success'
     ```
    
+4. 删除 Meta（将AskTable中保存的Meta删除）
+    ```python
+    In [18]: at.datasources.latest.meta.delete()
+    Out[18]: True
+    ```
+
 4. 查看示例问题
     ```python
     In [62]: ds = at.datasources.latest
     
     In [63]: ds.sample_questions
     Out[63]: '- 查询杭州房产的平均总价\n- 统计各区域房产数量 \n（为了方便沟通，您可以直接指定列名：城市, 区域, 子区域, 小区, 总价, 单价, 户型, 楼层, 建筑面积, 套内面积, 装修, 梯户比例, 电梯, 别墅类型, 挂牌日期, 产权, 房屋用途, 房龄, 链接）'
-
     ```
-5. 删除
+5. 删除数据源
 
-    ```python
-    In [64]: ds = at.datasources.latest
-    
-    In [65]: ds.delete()
+    ```python 
+    In [65]: ds = at.datasources.latest.delete()  # 删除最新的数据源
     Out[65]: True
-
     ```
 
 ### 开始对话
 
-1. 直接对数据源进行提问
+1. 快速提问
     ```python
-    In [52]: ds = at.datasources.latest
-    
-    In [53]: ds.ask("你好")
-    Out[53]: [OK-3s] 你好！请问有什么可以帮助您的吗？
+    In [8]: answer = at.datasources.latest.ask('你好')
+    Out[8]: [msg_5PvPIvwPofYG9HkUzjzzRc] [ai] [OK-10s] 你好！有什么可以帮助你的吗？   [Just now]>
     ```
 
-2. 创建一个对话，然后提问
+2. 选择多个数据源，一起提问
     ```python
-    In [47]: ds = at.datasources.latest
-    
-    In [48]: ds
-    Out[48]: <ds_2TDFez0l0qiSwFLsGvRKvV: 杭州房产信息 >
-    
-    In [49]: chat = at.chats.create([ds.id])
-    
-    In [50]: answer = chat.ask("你好")
-    
-    In [51]: answer.to_dict()
-    Out[51]: 
-    {'status': 'OK',
-     'elapsed_time': 2,
-     'answer_text': '你好！有什么可以帮助你的吗？',
-     'answer_file_url': None,
-     'answer_image_url': None,
-     'structure_queries': []}
+    In [9]: chat = at.chats.create(['ds_2MSqC5EUFawpK0nVOg8MXN', 'ds_34LANTyIJi0srCJrHFxRZ'])
+
+    In [10]: answer = chat.ask('你好')
+    
+    In [11]: answer.to_dict()
+    Out[11]: 
+    {'id': 'msg_4wQf4lD2AqDiPikRW4Tijk',
+     'chat_id': 'chat_G5g0FKbGQlljAKHrKz160',
+     'created': 1712287283,
+     'role': 'ai',
+     'content': {'status': 'OK',
+      'elapsed_time': 2,
+      'answer_text': '你好！有什么可以帮到您的吗？',
+      'answer_file_url': None,
+      'answer_image_url': None,
+      'structure_queries': None,
+      'statistics': None},
+     'reply_to_msg_id': 'msg_7aEuVCH8fX9BzlS3EvwKnl'}
     ```
    
 ### 查看对话
 
 1. 查看所有对话
     ```python
     In [40]: at.chats
```

## Comparing `asktable-0.13.5.dist-info/RECORD` & `asktable-0.13.6.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 asktable/__init__.py,sha256=12mHQ4Yrr7_vY7ilhc6_tdOtuclBWPWj88YFIiTTU6I,86
 asktable/api.py,sha256=e5XbDpNv3V0OVdpUin6xhkA1xW0u-Jik5wFlS_2ZkiY,2180
 asktable/cli.py,sha256=QDzk5ixcWzD0fT345a6jW7D8JXp9ps9xRLvXT4Uh-Kc,1667
-asktable/client.py,sha256=pfj9w1I5-0I78_maqds6pzS2OJmO7G8h_siwR09kUCw,1220
+asktable/client.py,sha256=D50bWrMySDdJdsLs-AxD1oHCeEpRn_4R2_x_XHaMWS8,887
 asktable/exceptions.py,sha256=HUys_aPowtoLqjO4XhANXBQCUUpty1zLxV4fWPqBkLc,41
 asktable/log.py,sha256=CIRsQtXMB24sD5fFsUn2hxlEVJB33Ej8TIQKyET2pQ8,875
 asktable/upload.py,sha256=ZctBvZtKnBZWucRXzvdp25zBNGz3eQ6PAfdhzLzfMUU,1492
 asktable/models/__init__.py,sha256=LDrDHxIjYwQN-fLW4BKMkuogEUcDfLxMYtlEkq0Xzfk,152
 asktable/models/base.py,sha256=vDe19yuK6SJglgRlz8Gq9qKg_x1Cxs2tK82VKeJ3UCI,2588
 asktable/models/chat.py,sha256=Z-2p_crBI5HwljcIxtRNV46_FfsVmNc5oRrwq3dNigE,2769
-asktable/models/datasource.py,sha256=RrRuWCKNtU-UXbNNm3wYuwB0riJpT8It44-mDf934tA,7670
+asktable/models/datasource.py,sha256=jIs4S3hcIESsbthwLqkvb3vPABnb0VgdyxQ4p7SMhPY,10178
 asktable/models/message.py,sha256=4bIt5VTBXvH5iQYb5_LoOIIbbNEkjmUsLfkjx4Il7-M,1264
 asktable/models/run.py,sha256=xf4oNReawEmQk0rmYn-uWPu_zkZD7ws1-77bOfUWYTw,878
 atcommon/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-atcommon/tools.py,sha256=s0C_ti20oi8HfeSuEn5Oo5hLY7Jjo0tlzi8hAcpyXyg,3352
-atcommon/version.py,sha256=SNfVf4udnrcFX9bEiktOPO5j8o5BsObUXEH7E7BK0B4,16
+atcommon/tools.py,sha256=TWlukRR8iaDY0FW4zM1hXO-2xM9NPcDB9PA4eYHipDI,3296
+atcommon/version.py,sha256=csXdmQRoJaLLMv4De_BYgUYYOu15o-wdPUt_seOcfOI,16
 atcommon/exceptions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 atcommon/exceptions/client.py,sha256=qHJfByYYiVOwdAs_L-R655tBmep6Kpg1ixvktBNQNA0,1282
 atcommon/exceptions/server.py,sha256=BSlYmdfDasoAq6oK9fyGhws3EBPII_hkHpx_JQmMdAY,95
 atcommon/exceptions/server_base.py,sha256=-X7xH1cmypIwl007CvuN2MIGqCv89sYb5jVi7s-Uzkc,2089
 atcommon/exceptions/server_plugins.py,sha256=MqQgXsGHUn0JB3EAos1vlf8SLjUMNGXAY7lcBKSA_8c,589
 atcommon/models/__init__.py,sha256=QDsebmmUAHCKrMs789lDMt9HU9DR0kH-TGnE1CtW9kU,466
 atcommon/models/base.py,sha256=UVM9dKL22-k4k44H6DYQeNIe35vJ1Vg0ppGDZiVDiIU,731
 atcommon/models/bi.py,sha256=77HEtXOEj2tJwXe34s5Shsu4LWDihxAols8Rcz5cOFU,2344
 atcommon/models/chat.py,sha256=Si_eo8H8h_NtIOc76qhXoYcHp_tPMavud2sEnUXEejo,1069
-atcommon/models/datasource.py,sha256=lNXrhSTLhk7EBqUgxgYgippHdWZTILIMCNJ2Hmv0e4A,1535
-atcommon/models/meta.py,sha256=mN-vlRo433lXAtoDKq7vTaEaw9u3LFdFBJc_EHptF2A,23566
-asktable-0.13.5.dist-info/METADATA,sha256=V1WSG4ZJQe0_ztEky-Vdbn-C-MxMo0wz9xuWTEMdGtw,8309
-asktable-0.13.5.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
-asktable-0.13.5.dist-info/entry_points.txt,sha256=OS7zXFo5f2wYttNLFf6EScGBaN_2XtYbV_Fl_C3RhY8,47
-asktable-0.13.5.dist-info/top_level.txt,sha256=j_dz8lioIJZMGAy7UUdXv0ytGGhh8HFsGhSXaULxT4M,18
-asktable-0.13.5.dist-info/RECORD,,
+atcommon/models/datasource.py,sha256=Qv-IIC9hDGXrWe5s__4U_f51QNDi9BCMJrf42cUJ5Tg,1424
+atcommon/models/meta.py,sha256=U5ZQxjlHbo62343nE_RCDz23OrN5vSL-YTOW_HdzuzQ,25728
+asktable-0.13.6.dist-info/METADATA,sha256=ZnIf8Mub7ZhNqyS0HfstYnDaToRJEJHgbas7xQB_D9g,9875
+asktable-0.13.6.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
+asktable-0.13.6.dist-info/entry_points.txt,sha256=OS7zXFo5f2wYttNLFf6EScGBaN_2XtYbV_Fl_C3RhY8,47
+asktable-0.13.6.dist-info/top_level.txt,sha256=j_dz8lioIJZMGAy7UUdXv0ytGGhh8HFsGhSXaULxT4M,18
+asktable-0.13.6.dist-info/RECORD,,
```

